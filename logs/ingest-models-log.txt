[THREADING] OpenMP threads: 8
[THREADING] MKL threads: 8 (dynamic=0)
=== Universal Safetensor Ingester (Modular) ===
Directory: D:\Models\generation_models\DeepSeek-V3.2-Speciale
Model: DeepSeek-V3.2-Speciale
Threshold: 0.5

[0] Parsing model manifest (config.json, architecture detection)...
[MANIFEST] Parsed config for DeepSeek-V3.2-Speciale (DEEPSEEK)
[MANIFEST] Found 39 config atoms

[1] Parsing tokenizer: "D:\\Models\\generation_models\\DeepSeek-V3.2-Speciale\\tokenizer.json"
[TOKENIZER] Loaded 127741 BPE merges, 128000 vocab entries

[2.5] Parsing model metadata (config, tokenizer, special tokens)...

=== Parsing Model Metadata: DeepSeek-V3.2-Speciale ===
[CONFIG] Parsed 60 config atoms, model_type=deepseek_v32, vocab_size=129280
[TOKENIZER] Loaded 818 special tokens, 127741 BPE merges, 128000 vocab entries
[METADATA] Total: 60 config atoms, 128000 vocab tokens, 127741 merges, 818 special tokens
[VOCAB] Transferred 128000 token compositions to context
[3] Parsing sharded model index: "D:\\Models\\generation_models\\DeepSeek-V3.2-Speciale\\model.safetensors.index.json"
[INFO] Found 2045 tensors
[3.1] Categorizing tensors for extraction...
[INFO] Created 2045 extraction plans

+--------------------------------------------------------------+
|              MODEL MANIFEST SUMMARY                          |
+--------------------------------------------------------------+
  Model: DeepSeek-V3.2-Speciale
  Architecture: DEEPSEEK
  Path: D:\Models\generation_models\DeepSeek-V3.2-Speciale
+--------------------------------------------------------------+
  DIMENSIONS:
    Vocab Size: 129280
    Model Dim (d_model): 7168
    Layers: 61
    Attention Heads: 128
    FFN Dim: 18432
    MoE Experts: 256 (top-8)
+--------------------------------------------------------------+
  TENSORS BY CATEGORY:
    Embeddings:    19 (eigenmap extraction)
    Attention:     10 (relation extraction)
    FFN:           1931 (relation extraction)
    Normalization: 23 (skipped)
    Convolution:   0 (skipped)
    Detection:     0 (eigenmap extraction)
    Other:         55
    TOTAL:         2045
+--------------------------------------------------------------+
  METADATA FOR INGESTION:
    Config atoms:    39
    Tokenizer atoms: 0
    BPE merges:      0 (composition edges)
    Vocab entries:   0
+--------------------------------------------------------------+


[3.5] Inserting model metadata as content...

=== Inserting Model Metadata: DeepSeek-V3.2-Speciale ===
[METADATA] Type: deepseek_v32
[METADATA] Config atoms: 60
[METADATA] BPE merges: 127741
[METADATA] Special tokens: 818
[METADATA] Vocab tokens: 128000
[METADATA] Loading 260 atoms (max codepoint: 65372 / 0xff5c)...
[CACHE] Loaded 260 atoms for 260 unique codepoints in 8 ms
[METADATA] Loaded 260 atoms
[CONFIG-AST] Built 120 nodes, 77 relations
[BPE-TREE] Built 128000 token compositions
[BPE-TREE] Built 255482 merge relations
[SPECIAL] Built 818 special token nodes
[METADATA] Total: 128896 compositions, 256377 relations
[STREAM] Dropping idx_comp_label to prevent corruption...
[STREAM] Buffers: comp=21896KB, child=116947KB, rel=45820KB
[STREAM] Merging into main tables...
[STREAM] Child diagnostics: 855047 total, 855047 have parent comp, 855047 have atom child
[STREAM] Inserted: 128896 compositions, 833212 children, 255789 relations
[STREAM] Recreating idx_comp_label...

[4] Building tensor name hierarchy...
[HIER] Building tensor hierarchy from 2045 tensors
[HIER] Found 37 unique codepoints in tensor names
[HIER] Validated 37 atoms exist
[HIER] Found 3435 unique hierarchy nodes
[HIER] Built 3435 compositions with atom children
[HIER] Built 3434 composition->composition edges
[HIER] Inserted/updated 3435 hierarchy compositions
NOTICE:  table "tmp_hier_atom_child" does not exist, skipping
[HIER] Inserted 154566 atom children
NOTICE:  table "tmp_hier_comp_child" does not exist, skipping
[HIER] Inserted 3434 composition->composition edges

[5] Inserting token compositions...
[COMP] Inserting 128000 token compositions...
[COMP] 128000 token compositions to insert
[COMP] Building batch strings with 16 threads...
  [BUILD] 128000/128000 tokens (247104/s)

[COMP] Built 81683KB compositions + 114284KB children in 518ms
[COMP] Streaming to database...
[COMP] Dropping idx_comp_label to prevent corruption...
[COMP] Creating temp tables...
[COMP] Copying compositions to temp table...
  [COPY] Batch 1/16 (5544KB)
  [COPY] Batch 2/16 (4221KB)
  [COPY] Batch 3/16 (3652KB)
  [COPY] Batch 4/16 (5448KB)
  [COPY] Batch 5/16 (5836KB)
  [COPY] Batch 6/16 (5209KB)
  [COPY] Batch 7/16 (5672KB)
  [COPY] Batch 8/16 (4845KB)
  [COPY] Batch 9/16 (6200KB)
  [COPY] Batch 10/16 (4342KB)
  [COPY] Batch 11/16 (5037KB)
  [COPY] Batch 12/16 (4603KB)
  [COPY] Batch 13/16 (5330KB)
  [COPY] Batch 14/16 (5584KB)
  [COPY] Batch 15/16 (5415KB)
  [COPY] Batch 16/16 (4736KB)

[COMP] Copying composition children to temp table...
  [COPY] Batch 1/16 (7677KB)
  [COPY] Batch 2/16 (5998KB)
  [COPY] Batch 3/16 (5185KB)
  [COPY] Batch 4/16 (7554KB)
  [COPY] Batch 5/16 (8103KB)
  [COPY] Batch 6/16 (7288KB)
  [COPY] Batch 7/16 (7874KB)
  [COPY] Batch 8/16 (6725KB)
  [COPY] Batch 9/16 (8652KB)
  [COPY] Batch 10/16 (6100KB)
  [COPY] Batch 11/16 (7090KB)
  [COPY] Batch 12/16 (6494KB)
  [COPY] Batch 13/16 (7512KB)
  [COPY] Batch 14/16 (7790KB)
  [COPY] Batch 15/16 (7599KB)
  [COPY] Batch 16/16 (6635KB)

[COMP] Inserting into composition table...
[COMP] Inserted 123539 compositions
[COMP] Inserting into composition_child table...
[COMP] Inserted 149 children
[COMP] Recreating idx_comp_label...
[COMP] Inserted 123539 compositions, 149 children in 8176ms

[5.5] Projecting token embeddings to 4D semantic coordinates...
============================================================
[PROJECTION] LAPLACIAN EIGENMAP PROJECTION starting
============================================================
[PROJECTION] Using config-driven tensor lookup
[PROJECTION] Scanning 2045 extraction plans...
[PROJECTION]   Candidate 1: model.layers.2.self_attn.kv_b_proj.weight [32768 x 512] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.2.self_attn.kv_b_proj.weight
[PROJECTION]   Candidate 2: model.embed_tokens.weight [129280 x 7168] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.embed_tokens.weight
[PROJECTION]   Candidate 3: model.layers.0.self_attn.q_b_proj.weight [24576 x 1536] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.0.self_attn.q_b_proj.weight
[PROJECTION]   Candidate 4: model.layers.1.self_attn.kv_b_proj.weight [32768 x 512] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.1.self_attn.kv_b_proj.weight
[PROJECTION]   Candidate 5: model.layers.0.self_attn.kv_b_proj.weight [32768 x 512] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.0.self_attn.kv_b_proj.weight
[PROJECTION]   Candidate 6: model.layers.2.self_attn.q_b_proj.weight [24576 x 1536] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.2.self_attn.q_b_proj.weight
[PROJECTION]   Candidate 7: model.layers.0.mlp.gate_proj.weight [18432 x 7168] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.0.mlp.gate_proj.weight
[PROJECTION]   Candidate 8: model.layers.0.mlp.up_proj.weight [18432 x 7168] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.0.mlp.up_proj.weight
[PROJECTION]   Candidate 9: model.layers.1.self_attn.q_b_proj.weight [24576 x 1536] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.1.self_attn.q_b_proj.weight
[PROJECTION]   Candidate 10: model.layers.1.mlp.gate_proj.weight [18432 x 7168] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.1.mlp.gate_proj.weight
[PROJECTION]   Candidate 11: model.layers.1.mlp.up_proj.weight [18432 x 7168] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.1.mlp.up_proj.weight
[PROJECTION]   Candidate 12: model.layers.2.mlp.gate_proj.weight [18432 x 7168] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.2.mlp.gate_proj.weight
[PROJECTION]   Candidate 13: model.layers.2.mlp.up_proj.weight [18432 x 7168] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.2.mlp.up_proj.weight
[PROJECTION]   Candidate 14: model.layers.3.self_attn.q_b_proj.weight [24576 x 1536] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.3.self_attn.q_b_proj.weight
[PROJECTION]   Candidate 15: model.layers.3.self_attn.kv_b_proj.weight [32768 x 512] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.3.self_attn.kv_b_proj.weight
[PROJECTION]   Candidate 16: model.layers.5.self_attn.q_b_proj.weight [24576 x 1536] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.5.self_attn.q_b_proj.weight
[PROJECTION]   Candidate 17: model.layers.5.self_attn.kv_b_proj.weight [32768 x 512] - AVAILABLE
[PROJECTION] >>> WILL PROJECT: model.layers.5.self_attn.kv_b_proj.weight
[PROJECTION] Processing embedding: model.layers.2.self_attn.kv_b_proj.weight
[PROJECTION] Loading 32768 x 512 embeddings...
[LOAD_TENSOR] Opening file: D:\Models\generation_models\DeepSeek-V3.2-Speciale\model-00001-of-000163.safetensors
[LOAD_TENSOR] Seeking to offset: 3102461536
[LOAD_TENSOR] Reading 16777216 elements as F8_E4M3
[LOAD_TENSOR] Successfully read 16777216 bytes (expected 16777216)
[LOAD_TENSOR] Non-zero values in first 100 elements: 100/100
[PROJECTION] Loaded in 35ms
[PROJECTION] Embedding statistics (raw from safetensor):
  Tensor: model.layers.2.self_attn.kv_b_proj.weight [32768 x 512]
  Dtype: F8_E4M3, File offset: 3102461536
  Min: -inf, Max: inf
  Mean: nan, StdDev: nan
  Zeros: 1313 (0.0078%)
  NaNs: 0
  First embedding (token 0): [3.0000e+00, 1.0400e+02, -1.1000e+01, -2.6000e+01, -1.4400e+02, 1.6250e+00, -1.0400e+02, 4.8000e+01, -8.0000e+01, 4.0000e+01, ...]
  Second embedding (token 1): [-5.2000e+01, -5.2000e+01, 1.2800e+02, 8.0000e+01, 1.2000e+01, -1.1200e+02, 1.7600e+02, 5.2000e+01, -6.0000e+00, , ...]
  First two rows identical: NO (good)
[PROJECTION] Projecting 32768 embeddings to 4D using Laplacian eigenmaps...

=== Laplacian Eigenmap Projection to 4D ===
Tokens: 32768, Embedding dim: 512

[1] Building k-NN similarity graph (k=15)...
[HNSWLIB] Building HNSW index for 32768 points, dim=512
[HNSWLIB] Using SEQUENTIAL strategy for 32768 points
[HNSWLIB] Adding points to index...
